{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a03ffa",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "\n",
    "Update the schema name if your Lakehouse uses a custom schema (default is `dbo`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ea3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# The folder where parquet files are stored in the Files section\n",
    "FILES_FOLDER = \"claims_data\"\n",
    "\n",
    "# Schema name (use 'dbo' for default, or your custom schema)\n",
    "SCHEMA_NAME = \"dbo\"\n",
    "\n",
    "# Tables to load\n",
    "TABLES = [\n",
    "    \"claims_history\",\n",
    "    \"claimant_profiles\",\n",
    "    \"fraud_indicators\",\n",
    "    \"regional_statistics\",\n",
    "    \"policy_claims_summary\",\n",
    "]\n",
    "\n",
    "print(f\"Source folder: Files/{FILES_FOLDER}/\")\n",
    "print(f\"Target schema: {SCHEMA_NAME}\")\n",
    "print(f\"Tables to load: {len(TABLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009e7fd",
   "metadata": {},
   "source": [
    "## Step 2: Verify Source Files\n",
    "\n",
    "Check that all parquet files exist in the Files section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f5a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List files in the source folder\n",
    "base_path = f\"Files/{FILES_FOLDER}\"\n",
    "\n",
    "print(f\"Checking files in {base_path}...\\n\")\n",
    "\n",
    "try:\n",
    "    files = mssparkutils.fs.ls(base_path)\n",
    "    parquet_files = [f.name for f in files if f.name.endswith('.parquet')]\n",
    "    \n",
    "    print(f\"Found {len(parquet_files)} parquet files:\")\n",
    "    for f in parquet_files:\n",
    "        print(f\"  ✅ {f}\")\n",
    "    \n",
    "    # Check for missing tables\n",
    "    missing = [t for t in TABLES if f\"{t}.parquet\" not in parquet_files]\n",
    "    if missing:\n",
    "        print(f\"\\n⚠️ Missing files for tables: {missing}\")\n",
    "        print(\"Run the upload_to_fabric.py script first.\")\n",
    "    else:\n",
    "        print(\"\\n✅ All required files found!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(f\"\\nMake sure the folder '{base_path}' exists and contains parquet files.\")\n",
    "    print(\"Run 'python upload_to_fabric.py' to upload the data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4bc2bc",
   "metadata": {},
   "source": [
    "## Step 3: Load Tables\n",
    "\n",
    "Load each parquet file as a managed Delta table. This will:\n",
    "- Read the parquet file from Files section\n",
    "- Write it as a Delta table in the Tables section\n",
    "- Overwrite if the table already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc08538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Load each table\n",
    "success_count = 0\n",
    "failed_tables = []\n",
    "\n",
    "for table_name in TABLES:\n",
    "    file_path = f\"{base_path}/{table_name}.parquet\"\n",
    "    full_table_name = f\"{SCHEMA_NAME}.{table_name}\" if SCHEMA_NAME else table_name\n",
    "    \n",
    "    print(f\"Loading {table_name}...\")\n",
    "    print(f\"   Source: {file_path}\")\n",
    "    print(f\"   Target: {full_table_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Read parquet file\n",
    "        df = spark.read.parquet(file_path)\n",
    "        row_count = df.count()\n",
    "        \n",
    "        # Write as Delta table (overwrites if exists)\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "        \n",
    "        print(f\" Loaded {row_count:,} rows\")\n",
    "        success_count += 1\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Failed: {e}\")\n",
    "        failed_tables.append(table_name)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Load complete: {success_count}/{len(TABLES)} tables\")\n",
    "if failed_tables:\n",
    "    print(f\"Failed: {', '.join(failed_tables)}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66eb5c9",
   "metadata": {},
   "source": [
    "## Step 4: Verify Tables\n",
    "\n",
    "Query each table to verify the data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12b16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" Table Summary\\n\")\n",
    "print(f\"{'Table':<25} {'Rows':>10} {'Columns':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for table_name in TABLES:\n",
    "    full_table_name = f\"{SCHEMA_NAME}.{table_name}\" if SCHEMA_NAME else table_name\n",
    "    try:\n",
    "        df = spark.table(full_table_name)\n",
    "        row_count = df.count()\n",
    "        col_count = len(df.columns)\n",
    "        print(f\"{table_name:<25} {row_count:>10,} {col_count:>10}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{table_name:<25} {'ERROR':>10} - {e}\")\n",
    "\n",
    "print(\"\\n Tables are ready for the Data Agent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89af372f",
   "metadata": {},
   "source": [
    "## Step 5: Sample Queries\n",
    "\n",
    "Test a few sample queries to ensure the data is accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8cb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query: Claims by type\n",
    "print(\" Claims by Type:\\n\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT claim_type, COUNT(*) as count, ROUND(AVG(estimated_damage), 2) as avg_damage\n",
    "    FROM {SCHEMA_NAME}.claims_history\n",
    "    GROUP BY claim_type\n",
    "    ORDER BY count DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query: High-risk claimants\n",
    "print(\" High-Risk Claimants (risk_score > 70):\\n\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT claimant_id, name, risk_score, claim_frequency, total_claims_count\n",
    "    FROM {SCHEMA_NAME}.claimant_profiles\n",
    "    WHERE risk_score > 70\n",
    "    ORDER BY risk_score DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f96b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query: Fraud rates by state\n",
    "print(\" Fraud Rates by State:\\n\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT state, ROUND(AVG(fraud_rate), 2) as avg_fraud_rate, SUM(total_claims) as total_claims\n",
    "    FROM {SCHEMA_NAME}.regional_statistics\n",
    "    GROUP BY state\n",
    "    ORDER BY avg_fraud_rate DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f6c154",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Open SQL Analytics Endpoint** - The tables are now accessible via SQL\n",
    "2. **Create Data Agent** - Add these tables to your Fabric Data Agent\n",
    "3. **Configure Agent Instructions** - Use the files in `agent_config/` folder\n",
    "4. **Publish Data Agent** - Make it available for connections\n",
    "5. **Create AI Foundry Connection** - Connect from Azure AI Foundry\n",
    "6. **Enable in App** - Set `USE_FABRIC_DATA_AGENT=true` in `.env`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
